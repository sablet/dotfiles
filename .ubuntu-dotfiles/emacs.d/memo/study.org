2016

* 02
** <2016-02-06 土 09:24>
	「論文の書き方」はほんとに文章とか表現を考えるうえでけっこう大事な気がする。特に文章を書くことは「脳内空間に並列に存在しているものに、時系列構造を与えることを意味する」とか。	

** <2016-02-07 日 16:19>
まし→さほど悪くない
{\normalsize foo} でfooのサイズが9px
他にもlarge,samll,hugeなど

** <2016-02-13 土 17:58>
   ダイヤモンド社からのzaiのFX予備校という本をひと通り読んだ。といっても実際には対して深入りするような本でもないので、裁量取引する場合にはどういったことに気をつけるのか何となく眺めた程度でしっかりと知識としては大してついていない。それでもこの本の目的としてなんとなくイメージを掴むというのはできたしまあいいのかも。

** <2016-02-13 土 18:01>
   論文の書き方、中小に関する章にて。今までのお勉強では抽象的な言葉が出てきて、それに対して記号問題ぐらいには対応で
きる程度のなんとなくの繋がりを掴む練習はしてきたといえる。けれども抽象的な言葉にはなんとなくよさげなイメージがあるために、どうしても意味もなく難しい言葉を使ってしまい結果として「言葉に振り回される」といった状態に多くの人がなりがちにあるらしい。またこういったものは何らかの手段で経験・具体的なものに変換しなければ意味がよくわからなくなるといったこともしばしばあるとか。こういった状態は文章を書いてしっかり時系列関係に書き下すことではっきり分かっているかどうかといったものが分かるらしい。実際卒論で散々苦労したり今までの勉強でどうもパッとしなかったのはこの辺に原因があるのかなとは思う。

** <2016-02-13 土 18:10>
実際には昨日の数理学研究科の焼肉会の話。まあ分野的に好奇心旺盛な人が集まっているということもあるのか、やたらとディープラーニングという単語は飛び交っていた。ただ元々はっきりしない言葉ということもあってか、実際にどういう処理が行われているかということにはあまり関心がなく、立木先生も含めてむしろそれによって何ができるか(画像分類とか)ということに関心がったよう。一般の人がそういう考え方なのはあらかじめわかっていたけど、多少でも情報の扱いに慣れている人（少なくとも俺よりはできるはず）でもそういった様子というのはちょっと意外だった。というか、個人的にはがっかりした。少なくとも現時点で「学習」にあたるのは最適化であると割と確信を持っているけども、なかなかそういう認識はあまりないらしい。まあそもそも自分のやってない分野で処理がどうしてるかなんてどうでもいいとか、ディープラーニングという単語のイメージが先走っていて機械学習専門


** <2016-02-20 土 09:20>
正規表現技術入門にて。もともとパーセプトロンモデルはAND,ORなどの論理演算を実現させるモデルとして導入されているらしい。たしかに生物学的なモデルには違いないが、これらの０−１パーセプトロンが正規表現に等しいこと、そして記憶領域を持たない計算モデルをこれで過不足なく定義することが可能であることが記述されている。そこから電子回路に使われるものが発展しているのだから信号処理技術そのものとほぼ等しのは無茶苦茶当たり前な気もする。これらを考えるとLSTMなどややこしいことをしなくても、文脈自由文法に似た形で記憶のモデルを再現してやれば時系列データをうまく処理するモデルを作り出すことができるかもしれない。本格的にやるなら、記憶の数理モデルを調べるとよさげ。

* 04

** <2016-04-15 金 19:30>
RNNの言語モデル(RNNLM)ではモデルと最適化をファイルとして保存してくれるよう。ソースを見ていくと、どうもnet.pyでネットワーク構造は記述されている。このようにネットワークのみを別に書いていくやり方もありなのか。D=numpy.load('rnnlm.model')でDにモデルクラスを格納することができた。これらのファイルは重みなどを最適化済のネットワーク構造であるよう。これなら使い方も割とわかるかもしれない。

最新のRNNの最適化手法
.clipped gradient
** <2016-04-22 金 20:42>
RNNの実装の形として以前のデータを入力として、入力の中のデータの最新データの一日後の最高値が一定異常上がる場合には買いのタイミング、そうでない場合には買いではないタイミングとして二値分類をすることができれば実装として十分に能力があると言えると思う。
